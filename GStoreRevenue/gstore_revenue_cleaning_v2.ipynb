{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import feather\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/train_v2.csv', nrows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"visits\": \"1\", \"hits\": \"1\", \"pageviews\": \"1\", \"bounces\": \"1\", \"newVisits\": \"1\", \"sessionQualityDim\": \"1\"}'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['totals'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json(df):\n",
    "    json_columns = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    for column in json_columns:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [column+\".\"+subcolumn for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_processing(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format = '%Y%m%d')\n",
    "    df[\"_weekday\"] = df['date'].dt.weekday\n",
    "    df[\"_day\"] = df['date'].dt.day \n",
    "    df[\"_month\"] = df['date'].dt.month\n",
    "    df[\"_year\"] = df['date'].dt.year\n",
    "    df[\"_visitHour\"] = pd.to_datetime(df[\"visitStartTime\"], unit = \"s\").dt.hour\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Converions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_numeric_bool_fillna(df, is_test_set = False):\n",
    "    df[\"totals.visits\"] = df[\"totals.visits\"].astype(int)\n",
    "    df[\"totals.hits\"] = df[\"totals.hits\"].astype(int)\n",
    "    df[\"totals.pageviews\"].fillna(1, inplace = True)\n",
    "    df[\"totals.pageviews\"] = df[\"totals.pageviews\"].astype(int)\n",
    "    df[\"totals.bounces\"].fillna(0, inplace=True)\n",
    "    df[\"totals.bounces\"] = df[\"totals.bounces\"].astype(int)\n",
    "    df[\"totals.newVisits\"].fillna(0, inplace=True)\n",
    "    df[\"totals.newVisits\"] = df[\"totals.newVisits\"].astype(int)\n",
    "    df[\"trafficSource.isTrueDirect\"].fillna(False, inplace = True)\n",
    "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n",
    "    df[\"totals.transactionRevenue\"].fillna(0.0, inplace=True)\n",
    "    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].astype(float)\n",
    "    if is_test_set:\n",
    "        df[\"totals.totalTransactionRevenue\"].fillna(0.0, inplace=True)\n",
    "        df[\"totals.totalTransactionRevenue\"] = df[\"totals.totalTransactionRevenue\"].astype(float)\n",
    "        df[\"totals.transactions\"].fillna(0, inplace=True)\n",
    "        df[\"totals.transactions\"] = df[\"totals.transactions\"].astype(int)\n",
    "        return df\n",
    "    else:\n",
    "        #df[\"totals.transactionRevenue\"].fillna(0.0, inplace=True)\n",
    "        #df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].astype(float)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably only need to log normalise the transaction revenue, but will create a function anyway, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_normalise(df, is_test_set):\n",
    "    if is_test_set == True:\n",
    "        return df\n",
    "    else:\n",
    "        df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].apply(lambda x: np.log1p(x))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Columns and hits column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_remove_cols(df, cols_to_drop = []):\n",
    "    if cols_to_drop == []:\n",
    "        constant_cols = [col for col in df.columns if df[col].nunique() == 1 and col != \"totals.visits\"]\n",
    "        null_cols =  [col for col in df.columns if df[col].isnull().sum()/len(df) > 0.5] \n",
    "        cols_to_drop = constant_cols + null_cols + ['hits','customDimensions']\n",
    "        df.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "        return df, cols_to_drop\n",
    "    else:\n",
    "        intersection = set(df.columns.tolist()).intersection(cols_to_drop)\n",
    "        df.drop(intersection, axis = 1, inplace = True)\n",
    "        return df, cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagging visitor ids as spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_flag_spender(df):\n",
    "    im_df = pd.DataFrame(df.groupby('fullVisitorId', as_index = False)['totals.transactionRevenue'].sum())\n",
    "    im_df.columns = ['fullVisitorId', 'totalUserRev']\n",
    "    im_df['spender'] = np.where(im_df['totalUserRev']>0.0,True,False)\n",
    "    df = df.merge(im_df, on = 'fullVisitorId')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_df(df, output_file_name):\n",
    "    feather.write_dataframe(df, output_file_name)\n",
    "    feather.write_dataframe(df.sample(frac=0.1, random_state = 1), output_file_name.split('.')[0]+\"_sample.\"+output_file_name.split('.')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining PreProcessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, is_test_set, cols_to_drop = []):\n",
    "    \n",
    "    df = convert_json(df)\n",
    "    \n",
    "    df = date_processing(df)\n",
    "    df = df_numeric_bool_fillna(df, is_test_set)\n",
    "    \n",
    "    df = df_normalise(df, is_test_set)\n",
    "    \n",
    "    df, cols_to_drop = df_remove_cols(df, cols_to_drop)\n",
    "    \n",
    "    return df, cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_load(raw_file_name, output_file_name, chunksize, is_test_set = False, cols_to_drop = []):\n",
    "    \n",
    "    df_proc = pd.DataFrame()\n",
    "    json_columns = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    df_reader = pd.read_csv(raw_file_name, converters = {column: json.loads for column in json_columns}, dtype = {'fullVisitorId':'str'}, chunksize = chunksize)\n",
    "    #chunksize works at 100,000. Smaller and not all the columns turn up in the json. \n",
    "    #probably a way to fix that later\n",
    "    \n",
    "    for chunk_id, df in enumerate(df_reader):\n",
    "        df.reset_index(drop=True, inplace=True) \n",
    "        df, cols_to_drop = process(df, is_test_set, cols_to_drop)\n",
    "        \n",
    "        df_proc = pd.concat([df_proc, df], axis = 0, sort=False).reset_index(drop=True)\n",
    "        \n",
    "        del df\n",
    "        \n",
    "        if chunk_id % 5 == 0:\n",
    "            print('{}: rows loaded: {}'.format(chunk_id, df_proc.shape[0]))\n",
    "\n",
    "    if is_test_set:\n",
    "        output_df(df_proc, output_file_name)\n",
    "        return df_proc\n",
    "    else:\n",
    "        df_proc = df_flag_spender(df_proc)\n",
    "        output_df(df_proc, output_file_name)\n",
    "        pickle.dump(cols_to_drop, open('data/cols_to_drop.pickle', 'wb'))\n",
    "        return df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rows loaded: 100000\n",
      "5: rows loaded: 600000\n",
      "10: rows loaded: 1100000\n",
      "15: rows loaded: 1600000\n"
     ]
    }
   ],
   "source": [
    "clean_train_df = df_load('data/train_v2.csv', 'data/clean_train_v2.feather', chunksize = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rows loaded: 100000\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = pickle.load(open('data/cols_to_drop_v2.pickle', 'rb'))\n",
    "clean_test_df = df_load('data/test_v2.csv', 'data/clean_test_v2.feather', chunksize = 100000, is_test_set = True, cols_to_drop = cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheating a bit with choosing a large enough chunksize to avoid the issue of the json columns not having every column in each chunk. \n",
    "Instead of having null values they just aren't in the json, possibly will need to come back and deal with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
