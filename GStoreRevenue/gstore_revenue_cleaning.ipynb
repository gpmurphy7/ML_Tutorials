{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import feather\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_json(path_to_csv):\n",
    "    json_columns = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    df = pd.read_csv(path_to_csv, converters = {column: json.loads for column in json_columns}, dtype = {'fullVisitorId':'str'})\n",
    "    for column in json_columns:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [column+\".\"+subcolumn for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_processing(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format = '%Y%m%d')\n",
    "    df[\"_weekday\"] = df['date'].dt.weekday\n",
    "    df[\"_day\"] = df['date'].dt.day \n",
    "    df[\"_month\"] = df['date'].dt.month\n",
    "    df[\"_year\"] = df['date'].dt.year\n",
    "    df[\"_visitHour\"] = pd.to_datetime(df[\"visitStartTime\"], unit = \"s\").dt.hour\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Converions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_numeric_bool_fillna(df, is_test_set = False):\n",
    "    df[\"totals.visits\"] = df[\"totals.visits\"].astype(int)\n",
    "    df[\"totals.hits\"] = df[\"totals.hits\"].astype(int)\n",
    "    df[\"totals.pageviews\"].fillna(1, inplace = True)\n",
    "    df[\"totals.pageviews\"] = df[\"totals.pageviews\"].astype(int)\n",
    "    df[\"totals.bounces\"].fillna(0, inplace=True)\n",
    "    df[\"totals.bounces\"] = df[\"totals.bounces\"].astype(int)\n",
    "    df[\"totals.newVisits\"].fillna(0, inplace=True)\n",
    "    df[\"totals.newVisits\"] = df[\"totals.newVisits\"].astype(int)\n",
    "    df[\"trafficSource.isTrueDirect\"].fillna(False, inplace = True)\n",
    "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n",
    "    if is_test_set:\n",
    "        return df\n",
    "    else:\n",
    "        df[\"totals.transactionRevenue\"].fillna(0.0, inplace=True)\n",
    "        df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably only need to log normalise the transaction revenue, but will create a function anyway, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_normalise(df):\n",
    "    df[\"totals.transactionRevenue\"] = df[\"totals.transactionRevenue\"].apply(lambda x: np.log1p(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_remove_cols(df, cols_to_drop = []):\n",
    "    if cols_to_drop == []:\n",
    "        constant_cols = [col for col in df.columns if df[col].nunique() == 1 and col != \"totals.visits\"]\n",
    "        null_cols =  [col for col in df.columns if df[col].isnull().sum()/len(df) > 0.5] \n",
    "        cols_to_drop = constant_cols+null_cols\n",
    "        df.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "    else:\n",
    "        df.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "    return df, cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagging visitor ids as spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_flag_spender(df):\n",
    "    im_df = pd.DataFrame(df.groupby('fullVisitorId', as_index = False)['totals.transactionRevenue'].sum())\n",
    "    im_df.columns = ['fullVisitorId', 'totalUserRev']\n",
    "    im_df['spender'] = np.where(im_df['totalUserRev']>0.0,True,False)\n",
    "    df = df.merge(im_df, on = 'fullVisitorId')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_df(df, output_file_name):\n",
    "    #pickle.dump(df, open(output_file_name, 'wb'))\n",
    "    feather.write_dataframe(df, output_file_name)\n",
    "    #pickle.dump(cols_to_drop, open('data/cols_to_drop.df', 'wb'))\n",
    "    feather.write_dataframe(df, 'data/cols_to_drop.feather')\n",
    "    #pickle.dump(df.sample(frac=0.1, random_state = 1), open(output_file_name.split('.')[0]+\"_sample.\"+output_file_name.split('.')[1], 'wb'))\n",
    "    feather.write_dataframe(df.sample(frac=0.1, random_state = 1), output_file_name.split('.')[0]+\"_sample.\"+output_file_name.split('.')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining PreProcessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pre_process_and_dump(raw_file_name, output_file_name, is_test_set = False, cols_to_drop = []):\n",
    "    df = read_csv_with_json(raw_file_name)\n",
    "    \n",
    "    df = date_processing(df)\n",
    "    \n",
    "    df = df_numeric_bool_fillna(df, is_test_set)\n",
    "    \n",
    "    df = df_normalise(df)\n",
    "    \n",
    "    df, cols_to_drop = df_remove_cols(df)\n",
    "    \n",
    "    if is_test_set:\n",
    "        output_df(df, output_file_name)\n",
    "        return df\n",
    "    else:\n",
    "        df = df_flag_spender(df)\n",
    "        output_df(df, output_file_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df = df_pre_process_and_dump('data/train.csv', 'data/clean_train.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
